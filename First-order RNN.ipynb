{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec925b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 5693\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eea9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_val_test_file = np.load('./data/g2_train_val_test_data_lstar.npz')\n",
    "\n",
    "\n",
    "x_train = train_val_test_file['train_x']\n",
    "m_train = np.sum(train_val_test_file['train_m'], axis=1)\n",
    "y_train = train_val_test_file['train_y']\n",
    "\n",
    "x_val = train_val_test_file['val_x']\n",
    "m_val = np.sum(train_val_test_file['val_m'], axis=1)\n",
    "y_val = train_val_test_file['val_y']\n",
    "\n",
    "x_test = train_val_test_file['test_x']\n",
    "m_test = np.sum(train_val_test_file['test_m'], axis=1)\n",
    "y_test = train_val_test_file['test_y']\n",
    "\n",
    "\n",
    "one_hot_y = lambda t: [1 if i == t else 0 for i in range(2)]\n",
    "y_train, y_val, y_test = np.array([one_hot_y(y) for y in y_train]), np.array([one_hot_y(y) for y in y_val]), np.array([one_hot_y(y) for y in y_test])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b889c610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 2 236 160\n"
     ]
    }
   ],
   "source": [
    "size_input = int(tf.shape(x_train)[1])\n",
    "size_output = int(tf.shape(y_train)[1])\n",
    "number_of_train_examples = int(tf.shape(x_train)[0])\n",
    "number_of_test_examples = int(tf.shape(x_val)[0])\n",
    "print(size_input,size_output,number_of_train_examples,number_of_test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce77e8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b307da9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    def __init__(self, size_input, size_hidden, size_output, device=None):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \"\"\"\n",
    "        size_input: int, size of input layer\n",
    "        size_hidden: int, size of hidden layer\n",
    "        size_output: int, size of output layer\n",
    "        device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
    "        \"\"\"\n",
    "        self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
    "        size_input, size_hidden, size_output, device\n",
    "        \n",
    "        # Initialize weights between input layer and hidden layer 1\n",
    "        self.W_xh = tf.Variable(tf.random.normal([self.size_input, self.size_hidden], seed=seed, stddev=0.1))\n",
    "        # Initialize weights between hidden layer t-1 and hidden layer t\n",
    "        self.W_hh = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden], seed=seed, stddev=0.1))\n",
    "        self.b_h = tf.Variable(tf.random.normal([1, self.size_hidden], seed=seed))\n",
    "        # Initialize weights between hidden layer and output layer\n",
    "        self.W_hq = tf.Variable(tf.random.normal([self.size_hidden, self.size_output], seed=seed, stddev=0.1))\n",
    "        self.b_q = tf.Variable(tf.random.normal([1, self.size_output], seed=seed))\n",
    "\n",
    "        # Define variables to be updated during backpropagation\n",
    "        self.RNN_variables = [self.W_xh, self.W_hh, self.b_h, self.W_hq, self.b_q]\n",
    "\n",
    "    \n",
    "    def forward(self, X, m):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        X: Tensor, inputs\n",
    "        \"\"\"\n",
    "        if self.device is not None:\n",
    "            with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
    "                self.y = self.compute_output(X, m)\n",
    "        else:\n",
    "            self.y = self.compute_output(X, m)\n",
    "\n",
    "        return self.y\n",
    "  \n",
    "    def loss(self, y_pred, y_true, L1=0, L2=0):\n",
    "        '''\n",
    "        y_pred - Tensor of shape (batch_size, size_output)\n",
    "        y_true - Tensor of shape (batch_size, size_output)\n",
    "        '''\n",
    "        y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
    "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
    "        l2_penlty = (tf.nn.l2_loss(self.W_xh)+tf.nn.l2_loss(self.W_hh)+tf.nn.l2_loss(self.W_hq))*L2\n",
    "        loss_with_l2 = l2_penlty+tf.losses.categorical_crossentropy(y_true_tf, y_pred_tf)\n",
    "#         print(\"y_true_tf\",y_true_tf,\"y_pred_tf\",y_pred_tf)\n",
    "        return loss_with_l2\n",
    "  \n",
    "    def backward(self, X_train, y_train, m, learning_rate, L1=0, L2=0.1):\n",
    "        \"\"\"\n",
    "        backward pass\n",
    "        \"\"\"\n",
    "#         optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted = self.forward(X_train, m)\n",
    "            current_loss = self.loss(predicted, y_train, L2=L2)\n",
    "        grads = tape.gradient(current_loss, self.RNN_variables)\n",
    "        \n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, self.RNN_variables))\n",
    "        return current_loss, predicted\n",
    "        \n",
    "        \n",
    "    def compute_output(self, X, m):\n",
    "        \"\"\"\n",
    "        Custom method to obtain output tensor during forward pass\n",
    "        \"\"\"\n",
    "        # Cast X to float32\n",
    "        X_tf = tf.cast(X, dtype=tf.float32)\n",
    "        # Initialize hidden layer\n",
    "        h = tf.zeros((X_tf.shape[0], self.size_hidden))\n",
    "        for i in range(m):\n",
    "            h = X_tf[0][i] * self.W_xh + tf.matmul(h, self.W_hh) + self.b_h\n",
    "            h = tf.nn.relu(h)\n",
    "        \n",
    "        # Compute output\n",
    "        output = tf.matmul(h, self.W_hq) + self.b_q\n",
    "        #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n",
    "        #Second add tf.Softmax(output) and then return this variable\n",
    "        return tf.nn.softmax(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cac76aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Epoch = 1 - Training Cross Entropy:= 0.17788093372926875 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 18.81\n",
      "Number of Epoch = 2 - Training Cross Entropy:= 0.17180761240296444 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 18.12\n",
      "Number of Epoch = 3 - Training Cross Entropy:= 0.16657974760411148 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 18.00\n",
      "Number of Epoch = 4 - Training Cross Entropy:= 0.16203060796705343 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 17.98\n",
      "Number of Epoch = 5 - Training Cross Entropy:= 0.15803548845194154 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 18.71\n",
      "Number of Epoch = 6 - Training Cross Entropy:= 0.15449826192047636 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 17.95\n",
      "Number of Epoch = 7 - Training Cross Entropy:= 0.15134417000463454 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 18.15\n",
      "Number of Epoch = 8 - Training Cross Entropy:= 0.14851372928942663 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 18.01\n",
      "Number of Epoch = 9 - Training Cross Entropy:= 0.1459590620913748 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 17.94\n",
      "Number of Epoch = 10 - Training Cross Entropy:= 0.14364152035470737 - Training Accuracy:= 0.9788135593220338 - Test Accuracy:= 0.9875\n",
      "Time taken (in seconds): 18.13\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "HIDDEN_SIZE = 6\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "\n",
    "size_hidden = HIDDEN_SIZE\n",
    "size_input = 1\n",
    "size_output = int(tf.shape(y_train)[1])\n",
    "number_of_train_examples = int(tf.shape(x_train)[0])\n",
    "number_of_test_examples = int(tf.shape(x_val)[0])\n",
    "\n",
    "\n",
    "rnn_on_gpu = RNNModel(size_input, size_hidden, size_output, device='cpu')\n",
    "time_start = time.time()\n",
    "epoch = 1\n",
    "loss_diff,last_loss = 1,0\n",
    "\n",
    "while epoch <= NUM_EPOCHS and abs(loss_diff) > 0.00001:\n",
    "    loss_total_gpu = tf.zeros([1,1], dtype=tf.float32)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train, m_train)).shuffle(BATCH_SIZE+BATCH_SIZE//4, seed=epoch*(seed)).batch(BATCH_SIZE)\n",
    "    for inputs, outputs, m in train_ds:\n",
    "        cur_loss, preds = rnn_on_gpu.backward(inputs, outputs, m.numpy()[0], learning_rate=LEARNING_RATE)\n",
    "        loss_total_gpu += cur_loss\n",
    "        \n",
    "  # Calculate Accuracy\n",
    "#     train_accuracy, test_accuracy = tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.CategoricalAccuracy()\n",
    "#     train_accuracy.update_state(y_train, rnn_on_gpu.forward(x_train, m_train))\n",
    "#     test_accuracy.update_state(y_val, rnn_on_gpu.forward(x_val, m_train))\n",
    "    train_loss = np.sum(loss_total_gpu) / x_train.shape[0]\n",
    "    # acc\n",
    "    val_acc = 0\n",
    "    train_acc = 0\n",
    "    for i in range(len(x_train)):\n",
    "        predicts = rnn_on_gpu.forward([x_train[i]], m_train[i])\n",
    "        if (predicts[0][0] > predicts[0][1] and y_train[i][0] > y_train[i][1]) or (predicts[0][0] < predicts[0][1] and y_train[i][0] < y_train[i][1]):\n",
    "            train_acc += 1\n",
    "    for i in range(len(x_val)):\n",
    "        predicts = rnn_on_gpu.forward([x_val[i]], m_val[i])\n",
    "        if (predicts[0][0] > predicts[0][1] and y_val[i][0] > y_val[i][1]) or (predicts[0][0] < predicts[0][1] and y_val[i][0] < y_val[i][1]):\n",
    "            val_acc += 1\n",
    "#     test_loss = np.sum(rnn_on_gpu.loss(rnn_on_gpu.forward(x_val, m_train), y_val)) / x_val.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "    loss_diff = train_loss - last_loss\n",
    "    last_loss = train_loss\n",
    "    print(f'Number of Epoch = {epoch} - Training Cross Entropy:= {np.sum(loss_total_gpu) / x_train.shape[0]} - Training Accuracy:= {train_acc/len(x_train)} - Test Accuracy:= {val_acc/len(x_val)}')\n",
    "    time_taken = time.time() - time_start\n",
    "    print('Time taken (in seconds): {:.2f}'.format(time_taken))\n",
    "    time_start = time.time()\n",
    "    epoch += 1\n",
    "\n",
    "# record loss and accuracy for final test set:\n",
    "# final_test_loss = np.sum(rnn_on_gpu.loss(rnn_on_gpu.forward(x_final_test), y_final_test)) / x_final_test.shape[0]\n",
    "# final_test_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "# final_test_acc.update_state(y_final_test, rnn_on_gpu.forward(x_final_test))\n",
    "# result_for_ten.append([final_test_loss,final_test_acc.result().numpy()])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d2be1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_train)):\n",
    "#     if y_train[i] == []\n",
    "    print(train_val_test_file['train_y'][i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f23bb299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16, 19, 22, 22, 22, 16, 19, 13,  7, 13, 10, 19, 19, 19, 16,  7, 16,\n",
       "       19, 16, 25, 16, 19, 22, 19, 10, 16, 22, 16, 16, 22, 19,  7, 16,  1,\n",
       "       16,  4, 22, 22,  7, 13, 10, 19, 10, 19, 19, 19, 25, 22,  1, 13, 16,\n",
       "       16, 13, 25, 25,  4,  4,  7, 10,  7, 10, 13, 19, 22, 16, 16, 22, 10,\n",
       "       16, 22, 25,  4, 10, 13, 13, 25, 22, 22, 10, 10, 10,  7, 13,  4, 10,\n",
       "        7,  7, 25, 13, 10,  4,  4, 25,  4,  4, 25,  7, 10, 25, 13,  7, 16,\n",
       "       25, 10, 25,  7, 10, 25, 13, 19, 19, 13,  7, 16, 25, 13,  4, 16, 19,\n",
       "       13, 10, 10, 10,  4,  4, 10, 10,  7, 13,  7, 19, 10, 22, 13,  4,  4,\n",
       "       22,  7, 19, 25, 25, 13, 22, 25, 22, 25, 13, 16,  7,  7, 25, 25, 19,\n",
       "       22,  7,  7, 22, 13,  4,  4])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995e0573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
